name: Auto Scrape WaterLevel

on:
  schedule:
    # ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô 20:00 ‡πÑ‡∏ó‡∏¢ (UTC+7) = 13:00 UTC
    - cron: '0 13 * * *'
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      PYTHONUNBUFFERED: "1"
      CSV_OUT: waterlevel_report.csv

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # ‡πÉ‡∏ä‡πâ Chrome ‡∏û‡∏£‡πâ‡∏≠‡∏° Selenium Manager (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏á chromedriver ‡πÄ‡∏≠‡∏á)
      - name: Setup Google Chrome
        uses: browser-actions/setup-chrome@v1
      - name: Show Chrome version
        run: google-chrome --version || true

      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install selenium pandas google-api-python-client google-auth google-auth-httplib2

      - name: Run scraper
        run: |
          set -eux
          python scrap2.py
          test -f "${CSV_OUT}"

      - name: Upload artifact (CSV)
        if: always() && hashFiles(env.CSV_OUT) != ''
        uses: actions/upload-artifact@v4
        with:
          name: waterlevel_report
          path: ${{ env.CSV_OUT }}
          if-no-files-found: ignore
          retention-days: 7

      # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢:
      # - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ DRIVE_FILE_ID -> Overwrite ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° (My Drive/Shared Drive ‡∏Å‡πá‡πÑ‡∏î‡πâ)
      # - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÅ‡∏ï‡πà PURIPAT_ID ‡πÄ‡∏õ‡πá‡∏ô Shared Drive -> Create ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô
      # - ‡∏ñ‡πâ‡∏≤ PURIPAT_ID ‡πÄ‡∏õ‡πá‡∏ô My Drive ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° -> ‡πÅ‡∏à‡πâ‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ä‡∏£‡πå‡πÉ‡∏´‡πâ SA ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà DRIVE_FILE_ID)
      - name: Append & Upload to Google Drive (merge + dedup + fixed schema)
        if: success() && hashFiles('waterlevel_report.csv') != ''
        env:
          SERVICE_ACCOUNT_JSON: ${{ secrets.SERVICE_ACCOUNT }}   # JSON ‡∏Ç‡∏≠‡∏á Service Account (‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô)
          DRIVE_FILE_ID: ${{ secrets.DRIVE_FILE_ID }}            # (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ My Drive) fileId ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
          DRIVE_FOLDER_ID: ${{ secrets.PURIPAT_ID }}             # (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á; ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á My/Shared Drive
          CSV_OUT: waterlevel_report.csv
        run: |
          python - <<'PY'
          import os, io, json, sys
          import pandas as pd
          from google.oauth2.service_account import Credentials
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
          from googleapiclient.errors import HttpError

          SA = json.loads(os.environ["SERVICE_ACCOUNT_JSON"])
          FILE_ID = (os.environ.get("DRIVE_FILE_ID") or "").strip() or None
          FOLDER_ID = (os.environ.get("DRIVE_FOLDER_ID") or "").strip() or None
          CSV = os.environ.get("CSV_OUT", "waterlevel_report.csv")

          # ---- ‡∏™‡∏Ñ‡∏µ‡∏°‡∏≤ 9 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©) ----
          CANON = ["Station","Location","Time","Water_Level","Bank_Level","Gauge_Zero","Capacity_Percent","Status","Data_Time"]
          TH2EN = {
            "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ":"Station","‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á":"Location","‡πÄ‡∏ß‡∏•‡∏≤":"Time","‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥":"Water_Level",
            "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏•‡∏¥‡πà‡∏á":"Bank_Level","‡∏Ñ‡πà‡∏≤‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏™‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö":"Gauge_Zero","%‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∏‡∏ô‡πâ‡∏≥":"Capacity_Percent",
            "‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå":"Status","‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•":"Data_Time"
          }
          def normalize(df: pd.DataFrame) -> pd.DataFrame:
              df = df.rename(columns=TH2EN)
              df = df[[c for c in df.columns if c in CANON]]
              for c in CANON:
                  if c not in df.columns: df[c] = ""
              return df[CANON]

          # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡πÇ‡∏•‡∏Ñ‡∏±‡∏•
          new_df = normalize(pd.read_csv(CSV))

          creds = Credentials.from_service_account_info(SA, scopes=["https://www.googleapis.com/auth/drive"])
          drive = build("drive", "v3", credentials=creds, cache_discovery=False)

          def meta(fid):
              return drive.files().get(fileId=fid, fields="id,name,mimeType,driveId,size,parents", supportsAllDrives=True).execute()

          def download_csv(fid):
              m = meta(fid)
              mt = m["mimeType"]
              if mt.startswith("application/vnd.google-apps"):
                  # ‡πÄ‡∏õ‡πá‡∏ô Google Sheet -> export ‡πÄ‡∏õ‡πá‡∏ô CSV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ merge
                  req = drive.files().export(fileId=fid, mimeType="text/csv")
              else:
                  req = drive.files().get_media(fileId=fid)
              buf = io.BytesIO()
              dl = MediaIoBaseDownload(buf, req)
              done = False
              while not done:
                  _, done = dl.next_chunk()
              buf.seek(0)
              try: df = pd.read_csv(buf)
              except Exception:
                  buf.seek(0); df = pd.read_csv(buf, encoding="utf-8-sig")
              return normalize(df)

          def upload_update(fid, df):
              tmp = "__merged_upload.csv"
              df.to_csv(tmp, index=False, encoding="utf-8-sig")
              media = MediaFileUpload(tmp, mimetype="text/csv", resumable=True)
              drive.files().update(fileId=fid, media_body=media, supportsAllDrives=True).execute()
              print(f"‚úÖ Updated (append) fileId={fid}, rows={len(df)}")

          def create_new(folder_id, df):
              tmp = "__merged_upload.csv"
              df.to_csv(tmp, index=False, encoding="utf-8-sig")
              body = {"name": os.path.basename(CSV)}
              if folder_id: body["parents"] = [folder_id]
              media = MediaFileUpload(tmp, mimetype="text/csv", resumable=True)
              created = drive.files().create(body=body, media_body=media, fields="id", supportsAllDrives=True).execute()
              print(f"‚úÖ Created new fileId={created['id']}, rows={len(df)}")
              return created["id"]

          def escape_single_quotes(s): return s.replace("'", "\\'")
          def find_by_name_in_folder(name, folder_id):
              q = "name = '{}' and '{}' in parents and trashed = false".format(escape_single_quotes(name), folder_id)
              r = drive.files().list(q=q, fields="files(id,name,mimeType)", supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
              files = r.get("files", [])
              return files[0] if files else None

          # 1) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ FILE_ID -> ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏î‡∏¥‡∏°, ‡∏£‡∏ß‡∏°, ‡∏•‡∏ö‡∏ã‡πâ‡∏≥, ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏±‡∏ö
          if FILE_ID:
              old_df = download_csv(FILE_ID)
              merged = pd.concat([old_df, new_df], ignore_index=True)
              # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏ï‡∏≤‡∏° Station + Data_Time (+Time ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ß‡∏•‡∏≤)
              keys = [k for k in ["Station","Data_Time","Time"] if k in merged.columns]
              merged.drop_duplicates(subset=keys, keep="last", inplace=True)
              upload_update(FILE_ID, merged)
              sys.exit(0)

          # 2) ‡πÑ‡∏°‡πà‡∏°‡∏µ FILE_ID -> ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
          if not FOLDER_ID:
              print("‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà DRIVE_FILE_ID (‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°) ‡∏´‡∏£‡∏∑‡∏≠ PURIPAT_ID (‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á)", file=sys.stderr); sys.exit(1)

          try:
              fmeta = meta(FOLDER_ID)
          except HttpError as e:
              print(f"‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}", file=sys.stderr); sys.exit(1)
          is_shared = bool(fmeta.get("driveId"))

          existing = find_by_name_in_folder(os.path.basename(CSV), FOLDER_ID)
          if existing:
              fid = existing["id"]
              old_df = download_csv(fid)
              merged = pd.concat([old_df, new_df], ignore_index=True)
              keys = [k for k in ["Station","Data_Time","Time"] if k in merged.columns]
              merged.drop_duplicates(subset=keys, keep="last", inplace=True)
              upload_update(fid, merged)
              sys.exit(0)

          # 3) ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°:
          if is_shared:
              # Shared Drive ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ
              create_new(FOLDER_ID, new_df)
              sys.exit(0)
          else:
              # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô My Drive ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° -> SA ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
              print("‚ùå ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô My Drive ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ append; SA ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ '‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà' ‡πÉ‡∏ô My Drive ‡πÑ‡∏î‡πâ", file=sys.stderr)
              print("üëâ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ append ‡∏ö‡∏ô My Drive: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ä‡∏£‡πå‡πÉ‡∏´‡πâ SA ‡πÄ‡∏õ‡πá‡∏ô Editor ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà DRIVE_FILE_ID ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô", file=sys.stderr)
              sys.exit(1)
          PY
