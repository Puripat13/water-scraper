name: Auto Scrape Dam (Large & Medium)

on:
  schedule:
    - cron: '0 13 * * *'   # รันทุกวัน 20:00 น. (เวลาไทย, UTC+7)
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      TZ: Asia/Bangkok
      # รายชื่อไฟล์ผลลัพธ์จากสคริปต์ของคุณ
      CSV_LARGE: waterdam_report_large.csv
      CSV_MEDIUM: waterdam_report_medium.csv
      CSV_COMBINED: waterdam_report.csv

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Setup Google Chrome (for Selenium)
        uses: browser-actions/setup-chrome@v1

      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install selenium pandas google-api-python-client google-auth google-auth-httplib2

      - name: Run scraper (เก็บแหล่งน้ำ ขนาดใหญ่/กลาง)
        # ⬇️ เปลี่ยนชื่อไฟล์ให้ตรงกับสคริปต์ที่คุณบันทึก
        run: python dam_scraper.py

      - name: Combine "large" + "medium" → one CSV (waterdam_report.csv)
        env:
          CSV_LARGE: ${{ env.CSV_LARGE }}
          CSV_MEDIUM: ${{ env.CSV_MEDIUM }}
          CSV_COMBINED: ${{ env.CSV_COMBINED }}
        run: |
          python - <<'PY'
          import os
          import pandas as pd

          large = os.getenv("CSV_LARGE", "waterdam_report_large.csv")
          medium = os.getenv("CSV_MEDIUM", "waterdam_report_medium.csv")
          combined = os.getenv("CSV_COMBINED", "waterdam_report.csv")

          if not os.path.exists(large) and not os.path.exists(medium):
              raise SystemExit("❌ ไม่พบไฟล์ large และ medium จากสคริปต์")

          dfs = []
          if os.path.exists(large):
              dfl = pd.read_csv(large, dtype=str)
              dfl["Dam_Size"] = "Large"
              dfs.append(dfl)
          if os.path.exists(medium):
              dfm = pd.read_csv(medium, dtype=str)
              dfm["Dam_Size"] = "Medium"
              dfs.append(dfm)

          all_df = pd.concat(dfs, ignore_index=True, sort=False)

          # ลบแถวซ้ำแบบ all-columns (กันซ้ำจากการรันรายวัน)
          all_df.drop_duplicates(keep="last", inplace=True)

          # พยายามเรียงตามเวลา ถ้ามีคอลัมน์วันที่อยู่คอลัมน์สุดท้าย (ตามโค้ดคุณแนบวันที่ไว้ท้าย)
          # หรือมีคอลัมน์ชื่อ Date/DateTime
          date_cols = [c for c in all_df.columns if c.lower() in ("date","datetime","data_time","time","วันที่","เวลา")]
          try:
              if date_cols:
                  c = date_cols[-1]
                  all_df[c] = pd.to_datetime(all_df[c], errors="coerce", dayfirst=True)
                  all_df.sort_values(c, inplace=True)
          except Exception:
              pass

          all_df.to_csv(combined, index=False, encoding="utf-8-sig")
          print(f"✅ Combined saved → {combined}  rows={len(all_df)}")
          PY

      - name: Append & Upload CSVs to Google Drive (Shared Drive)
        env:
          DRIVE_FOLDER_ID: ${{ secrets.PURIPAT_ID }}      # ใส่ Folder ID ของ Shared Drive/โฟลเดอร์ปลายทาง
          SERVICE_ACCOUNT_JSON: ${{ secrets.SERVICE_ACCOUNT }}  # เนื้อ JSON ของ Service Account (ทั้งก้อน)
          CSV_LARGE: ${{ env.CSV_LARGE }}
          CSV_MEDIUM: ${{ env.CSV_MEDIUM }}
          CSV_COMBINED: ${{ env.CSV_COMBINED }}
        run: |
          python - <<'PY'
          import os, io, json, sys
          import pandas as pd
          from google.oauth2.service_account import Credentials
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

          FOLDER_ID = os.environ["DRIVE_FOLDER_ID"]
          SA_JSON = os.environ["SERVICE_ACCOUNT_JSON"]
          files_to_handle = [os.getenv("CSV_LARGE"), os.getenv("CSV_MEDIUM"), os.getenv("CSV_COMBINED")]
          files_to_handle = [f for f in files_to_handle if f and os.path.exists(f)]

          if not files_to_handle:
              print("❌ ไม่มีไฟล์ CSV ให้ส่งขึ้น Drive", file=sys.stderr)
              sys.exit(1)

          info = json.loads(SA_JSON)
          creds = Credentials.from_service_account_info(info, scopes=["https://www.googleapis.com/auth/drive"])
          drive = build("drive", "v3", credentials=creds, cache_discovery=False)

          # ตรวจสอบสิทธิ์เข้าถึงโฟลเดอร์ (รองรับ Shared Drive)
          drive.files().get(fileId=FOLDER_ID, fields="id,name,driveId", supportsAllDrives=True).execute()

          def find_existing(fid_folder, name):
              q = f"name = '{name.replace(\"'\",\"\\\\'\")}' and '{fid_folder}' in parents and trashed = false"
              res = drive.files().list(
                  q=q,
                  fields="files(id,name,mimeType)",
                  supportsAllDrives=True,
                  includeItemsFromAllDrives=True,
              ).execute()
              return res.get("files", [])

          for local_csv in files_to_handle:
              print(f"\n===== Upload/Append: {local_csv} =====")
              base = os.path.basename(local_csv)

              exist = find_existing(FOLDER_ID, base)
              new_df = pd.read_csv(local_csv, dtype=str)

              if exist:
                  fid = exist[0]["id"]
                  mime = exist[0]["mimeType"]

                  if mime.startswith("application/vnd.google-apps"):
                      print("❌ ไฟล์บนไดรฟ์เป็น Google Sheets (ต้องใช้ Sheets API) — ข้ามไฟล์นี้ก่อน", file=sys.stderr)
                      continue

                  # ดาวน์โหลดของเดิม
                  request = drive.files().get_media(fileId=fid)
                  fh = io.BytesIO()
                  downloader = MediaIoBaseDownload(fh, request)
                  done = False
                  while not done:
                      status, done = downloader.next_chunk()
                  fh.seek(0)
                  try:
                      old_df = pd.read_csv(fh, dtype=str)
                  except Exception:
                      fh.seek(0)
                      old_df = pd.read_csv(fh, dtype=str, encoding="utf-8-sig")

                  # รวม + ลบซ้ำ (all-columns)
                  merged = pd.concat([old_df, new_df], ignore_index=True, sort=False).drop_duplicates(keep="last")
                  tmp = "__tmp_upload.csv"
                  merged.to_csv(tmp, index=False, encoding="utf-8-sig")

                  media = MediaFileUpload(tmp, mimetype="text/csv", resumable=True)
                  drive.files().update(fileId=fid, media_body=media, supportsAllDrives=True).execute()
                  print(f"✅ Updated (append+dedup): {base}  rows={len(merged)}")

              else:
                  meta = {"name": base, "parents": [FOLDER_ID]}
                  media = MediaFileUpload(local_csv, mimetype="text/csv", resumable=True)
                  created = drive.files().create(
                      body=meta, media_body=media, fields="id", supportsAllDrives=True
                  ).execute()
                  print(f"✅ Created new: {base} (fileId={created['id']})")
          PY
