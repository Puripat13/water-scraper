# -*- coding: utf-8 -*-
import os, time, re, sys, math
from datetime import datetime
import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

URL = "https://nationalthaiwater.onwr.go.th/waterlevel"
CSV_OUT = "waterlevel_report.csv"

# ‡∏á‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ scrape (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) - ‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å ENV ‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô 600 = 10 ‡∏ô‡∏≤‡∏ó‡∏µ
SCRAPE_BUDGET_SEC = int(os.environ.get("SCRAPE_BUDGET_SEC", "600"))

def make_driver():
    opt = Options()
    opt.page_load_strategy = "eager"
    opt.add_argument("--headless=new")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--window-size=1366,840")
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument("--lang=th-TH,th")
    opt.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    )
    drv = webdriver.Chrome(options=opt)
    drv.set_page_load_timeout(60)
    return drv

# ---------- Tools around the table/paginator ----------
def find_rich_table_locator(driver):
    """‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ selector ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà '‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡πá‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏ß"""
    cands = [
        (By.CSS_SELECTOR, "div.MuiDataGrid-virtualScrollerRenderZone .MuiDataGrid-row"),
        (By.CSS_SELECTOR, "div[role='rowgroup'] [role='row']"),
        (By.CSS_SELECTOR, ".MuiTable-root tbody tr"),
        (By.CSS_SELECTOR, "table tbody tr"),
    ]
    best = None
    best_n = -1
    for by, sel in cands:
        try:
            WebDriverWait(driver, 6).until(EC.presence_of_element_located((by, sel)))
            n = len(driver.find_elements(by, sel))
            if n > best_n:
                best_n = n
                best = (by, sel)
        except Exception:
            pass
    if not best:
        raise TimeoutException("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤")
    print(f"[INFO] ‡πÉ‡∏ä‡πâ locator: {best} (‡πÅ‡∏ñ‡∏ß‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô {best_n})")
    return best

def find_paginator_root(driver):
    """‡∏´‡∏≤ paginator ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÇ‡∏ã‡∏ô‡πÅ‡∏™‡∏î‡∏á '1‚Äì100 of N' ‡πÅ‡∏•‡∏∞‡∏õ‡∏∏‡πà‡∏° next)"""
    sels = [
        ".MuiTablePagination-root",
        "div.MuiDataGrid-footerContainer",
        "div[class*='MuiTablePagination']",
    ]
    for sel in sels:
        els = driver.find_elements(By.CSS_SELECTOR, sel)
        for e in els:
            if e.is_displayed():
                return e
    return None

def set_rows_per_page_100(pager):
    """‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ï‡∏±‡πâ‡∏á Rows per page = 100 ‡∏ñ‡πâ‡∏≤‡πÄ‡∏°‡∏ô‡∏π‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ô‡∏µ‡πâ"""
    try:
        # ‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏°‡∏ô‡∏π page-size ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô div[role=button] ‡∏´‡∏£‡∏∑‡∏≠ .MuiSelect-select
        triggers = pager.find_elements(By.XPATH, ".//*[(@role='button' or contains(@class,'MuiSelect-select'))]")
        for trg in triggers:
            try:
                trg.click()
                time.sleep(0.2)
                items = pager.find_elements(By.XPATH, "//li[normalize-space(.)='100']")
                if not items:
                    # ‡πÄ‡∏°‡∏ô‡∏π‡∏≠‡∏≤‡∏à‡∏ß‡∏≤‡∏á‡∏ô‡∏≠‡∏Å pager (‡πÄ‡∏õ‡πá‡∏ô portal)
                    items = pager._parent.find_elements(By.XPATH, "//li[normalize-space(.)='100']")
                if items:
                    items[0].click()
                    print("[INFO] ‡∏ï‡∏±‡πâ‡∏á Rows/page = 100")
                    time.sleep(0.5)
                    return
            except Exception:
                continue
    except Exception:
        pass

def get_display_text(pager):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° '1‚Äì100 of N' (‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á)"""
    sels = [
        ".MuiTablePagination-displayedRows",
        "*[class*='MuiTablePagination-displayedRows']",
        "//p[contains(.,' of ')]",
    ]
    for sel in sels:
        try:
            if sel.startswith("//"):
                els = pager._parent.find_elements(By.XPATH, sel)
            else:
                els = pager.find_elements(By.CSS_SELECTOR, sel)
            for e in els:
                t = e.text.strip()
                if t:
                    return t
        except Exception:
            continue
    return ""

def parse_total_from_display(txt):
    """
    ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏£‡∏ß‡∏° N ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á '1‚Äì100 of 1234'
    ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô (start, end, total) ‡∏ñ‡πâ‡∏≤‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ, ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡∏Ñ‡∏∑‡∏ô (None, None, None)
    """
    if not txt:
        return None, None, None
    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    nums = [int(x) for x in re.findall(r"\d+", txt)]
    if len(nums) >= 3:
        # ‡πÄ‡∏î‡∏≤‡∏™‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô start, end, total
        return nums[0], nums[1], nums[2]
    if len(nums) == 1:
        return None, None, nums[0]
    return None, None, None

def find_next_in_pager(pager):
    """‡∏´‡∏≤ next ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô pager ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏Å‡∏±‡∏ô‡∏Å‡∏î‡∏ú‡∏¥‡∏î component)"""
    xps = [
        ".//button[@aria-label='Next page' and not(@disabled)]",
        ".//span[@title='Next Page']/button[not(@disabled)]",
        ".//button[not(@disabled) and (contains(., 'Next') or contains(., '‡∏ñ‡∏±‡∏î‡πÑ‡∏õ'))]",
    ]
    for xp in xps:
        els = pager.find_elements(By.XPATH, xp)
        for el in els:
            if el.is_displayed() and el.is_enabled():
                return el
    return None

def first_row_signature(driver, locator, k=3):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏≤‡∏¢‡πÄ‡∏ã‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏≠‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ö‡∏ô‡∏™‡∏∏‡∏î k ‡πÅ‡∏ñ‡∏ß‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏£‡∏¥‡∏á"""
    by, sel = locator
    rows = driver.find_elements(by, sel)[:k]
    parts = []
    for r in rows:
        cells = r.find_elements(By.CSS_SELECTOR, "td") or r.find_elements(By.CSS_SELECTOR, "[role='gridcell']")
        parts.append("|".join([c.text.strip() for c in cells]))
    return "¬ß".join(parts)

def extract_cells_from_row(r):
    tds = r.find_elements(By.CSS_SELECTOR, "td")
    if not tds:
        tds = r.find_elements(By.CSS_SELECTOR, "[role='gridcell']")
    return [c.text.strip() for c in tds if c.text is not None]

def get_rows(driver, locator):
    by, sel = locator
    rows = []
    for r in driver.find_elements(by, sel):
        cols = extract_cells_from_row(r)
        if cols:
            rows.append(cols)
    return rows

# ---------- Clean & Save ----------
def extract_thai(text):
    if pd.isna(text) or text is None:
        return ""
    m = re.search(r"[‡∏Å-‡πô].*", str(text))
    return m.group(0).strip() if m else str(text).strip()

def save_csv(rows):
    if not rows:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        return
    headers = [
        "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ","‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á","‡πÄ‡∏ß‡∏•‡∏≤","‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥",
        "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏•‡∏¥‡πà‡∏á","‡∏Ñ‡πà‡∏≤‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏™‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö","%‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∏‡∏ô‡πâ‡∏≥",
        "‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå","‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    ]
    fixed = []
    for r in rows:
        r = (list(r) + [""] * 9)[:9]
        fixed.append(r)
    df = pd.DataFrame(fixed, columns=headers)
    df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"] = df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"].apply(extract_thai)
    df = df.replace({"-": "", "‚Äì": ""})

    file_exists = os.path.exists(CSV_OUT)
    df.to_csv(CSV_OUT, mode="a", index=False, encoding="utf-8-sig", header=not file_exists)
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(df)} ‡πÅ‡∏ñ‡∏ß -> {CSV_OUT}")

# ---------- Main scrape ----------
def scrape_all():
    d = make_driver()
    t0 = time.time()
    try:
        print(f"[INFO] ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤: {URL}")
        d.get(URL)
        WebDriverWait(d, 30).until(EC.presence_of_element_located((By.XPATH, "//*")))
        time.sleep(1.0)

        locator = find_rich_table_locator(d)
        pager = find_paginator_root(d)
        if pager:
            set_rows_per_page_100(pager)
            time.sleep(0.5)

        # ‡∏≠‡πà‡∏≤‡∏ô total ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡∏¢
        total_start, total_end, total_n = (None, None, None)
        page_size = 100
        pages_plan = None
        if pager:
            txt = get_display_text(pager)
            total_start, total_end, total_n = parse_total_from_display(txt)
            if total_n:
                if total_start and total_end:
                    page_size = max(1, total_end - total_start + 1)
                pages_plan = max(1, math.ceil(total_n / page_size))
                print(f"[INFO] paginator: {txt} -> total={total_n}, page_size={page_size}, pages={pages_plan}")

        all_rows = []
        curdate = datetime.today().strftime("%d/%m/%Y")
        page = 1
        # ‡πÉ‡∏ä‡πâ signature ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏£‡∏¥‡∏á
        sig = first_row_signature(d, locator)

        while True:
            # ‡∏á‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
            if time.time() - t0 > SCRAPE_BUDGET_SEC:
                print(f"[WARN] ‡∏´‡∏°‡∏î‡∏á‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ {SCRAPE_BUDGET_SEC}s -> ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πà‡∏≠")
                break

            time.sleep(0.4)
            rows = get_rows(d, locator)
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö 9 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå + ‡πÉ‡∏™‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            for r in rows:
                r = (r + [""] * 9)[:9]
                r[-1] = curdate
                all_rows.append(r)

            # ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            if pager:
                # ‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß -> ‡∏à‡∏ö
                if pages_plan and page >= pages_plan:
                    print(f"[INFO] ‡∏Ñ‡∏£‡∏ö {page}/{pages_plan} ‡∏´‡∏ô‡πâ‡∏≤ -> ‡∏à‡∏ö")
                    break
                nxt = find_next_in_pager(pager)
            else:
                nxt = None

            if not nxt:
                print("[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏∏‡πà‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÉ‡∏ô paginator -> ‡∏à‡∏ö")
                break

            # ‡∏Ñ‡∏•‡∏¥‡∏Å next ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏à‡∏ô signature ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
            old_sig = sig
            try:
                d.execute_script("arguments[0].click();", nxt)
                # ‡∏£‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (signature ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô / displayedRows ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
                changed = False
                for _ in range(30):  # ~3s
                    time.sleep(0.1)
                    sig = first_row_signature(d, locator)
                    if sig and sig != old_sig:
                        changed = True
                        break
                if not changed:
                    print("[WARN] ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô -> ‡∏à‡∏ö")
                    break
                page += 1
                if pager:
                    print(f"[INFO] ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ {page} / {pages_plan or '?'}")
            except Exception as e:
                print(f"[WARN] ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e} -> ‡∏à‡∏ö")
                break

        # ‡∏Ñ‡∏±‡∏î‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏Å‡∏±‡∏ô header ‡πÅ‡∏ù‡∏á)
        cleaned = [r for r in all_rows if re.search(r"[‡∏Å-‡πô]", r[0] if r else "")]
        return cleaned, t0
    finally:
        d.quit()

def main():
    try:
        rows, t0 = scrape_all()
        save_csv(rows)
        print(f"‚è± ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {time.time() - t0:.2f}s, ‡∏£‡∏ß‡∏° {len(rows)} ‡πÅ‡∏ñ‡∏ß")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
