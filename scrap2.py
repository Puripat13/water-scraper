# -*- coding: utf-8 -*-
"""
Scrape: https://nationalthaiwater.onwr.go.th/waterlevel
Append -> waterlevel_report.csv (utf-8-sig)
NOTE: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Drive (‡πÉ‡∏´‡πâ workflow ‡∏ó‡∏≥)
"""
import os, time, re
from datetime import datetime
import pandas as pd

# -------- Selenium --------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# ================== CONFIG ==================
URL = "https://nationalthaiwater.onwr.go.th/waterlevel"
CSV_OUT = os.environ.get("CSV_OUT", "waterlevel_report.csv")  # allow override from workflow
PAGELOAD_TIMEOUT = int(os.environ.get("PAGELOAD_TIMEOUT", "120"))
WAIT_SEC = float(os.environ.get("WAIT_SEC", "1.2"))
FIRST_WAIT = int(os.environ.get("FIRST_WAIT", "60"))  # wait for first table load
# ===================================================


def make_driver():
    opt = Options()
    # ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö GitHub Actions / headless server
    opt.add_argument("--headless=new")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--window-size=1920,1080")
    opt.add_argument("--disable-software-rasterizer")
    opt.add_argument("--disable-extensions")
    opt.add_argument("--disable-features=Translate,BackForwardCache,MediaRouter")
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument("--remote-allow-origins=*")
    opt.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    )

    drv = webdriver.Chrome(options=opt)  # Selenium Manager ‡∏à‡∏∞‡∏´‡∏≤‡πÑ‡∏î‡∏£‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á
    drv.set_page_load_timeout(PAGELOAD_TIMEOUT)
    drv.set_script_timeout(PAGELOAD_TIMEOUT)
    return drv


def _safe_navigate(driver, url: str):
    """‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á renderer timeout: ‡πÉ‡∏ä‡πâ CDP navigate + fallback"""
    try:
        # ‡πÄ‡∏õ‡∏¥‡∏î Page domain ‡∏Å‡πà‡∏≠‡∏ô
        driver.execute_cdp_cmd("Page.enable", {})
        driver.execute_cdp_cmd("Page.navigate", {"url": url})
    except Exception:
        # fallback
        driver.get(url)


def scrape_waterlevel():
    driver = make_driver()
    start_time = time.time()
    try:
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏£‡∏≠ render ‡∏´‡∏ô‡∏±‡∏Å ‡πÜ
        _safe_navigate(driver, URL)

        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î FIRST_WAIT ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        WebDriverWait(driver, FIRST_WAIT).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".MuiTable-root tbody tr"))
        )

        all_data = []
        current_date = datetime.today().strftime("%d/%m/%Y")

        while True:
            time.sleep(WAIT_SEC)

            rows = driver.find_elements(By.CSS_SELECTOR, ".MuiTable-root tbody tr")
            for row in rows:
                cols = [c.text.strip() for c in row.find_elements(By.CSS_SELECTOR, "td")]
                if len(cols) < 5:
                    continue
                # ‡∏ö‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏°‡∏µ "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
                if len(cols) == 9:
                    cols[-1] = current_date
                else:
                    cols.append(current_date)
                all_data.append(cols)

            # ‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° next
            try:
                next_btn = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, "//span[@title='Next Page']/button"))
                )
            except TimeoutException:
                break  # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏∏‡πà‡∏° ‚Üí ‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

            # ‡∏ñ‡πâ‡∏≤ disabled ‡∏Å‡πá‡∏à‡∏ö
            try:
                disabled = next_btn.get_attribute("disabled")
                if disabled in ("true", True, "disabled"):
                    break
            except Exception:
                pass

            # ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏î‡πâ‡∏ß‡∏¢ JS ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå (‡∏Å‡∏±‡∏ô overlay/obstruction)
            try:
                driver.execute_script("arguments[0].click();", next_btn)
                print("‚û°Ô∏è Next Page")
            except WebDriverException:
                # ‡∏•‡∏≠‡∏á scroll ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏•‡∏¥‡∏Å‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
                time.sleep(0.3)
                driver.execute_script("arguments[0].click();", next_btn)

            # ‡∏£‡∏≠‡πÉ‡∏´‡πâ page ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".MuiTable-root tbody tr"))
            )

        return all_data, start_time

    except TimeoutException as e:
        # log ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏¢‡∏ô‡∏ï‡πà‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ workflow fail (‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ä‡∏±‡∏î)
        print(f"‚ö†Ô∏è Timeout while loading or waiting elements: {e}")
        raise
    finally:
        driver.quit()


# ----- helper: ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ -----
def extract_thai(text: str) -> str:
    if pd.isna(text) or text is None:
        return ""
    m = re.search(r"[‡∏Å-‡πô].*", str(text))
    return m.group(0).strip() if m else ""


def save_csv(all_data):
    if not all_data:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (all_data ‡∏ß‡πà‡∏≤‡∏á) ‚Äî ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå")
        return 0

    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
    max_cols = max(len(r) for r in all_data)
    all_data = [r + [''] * (max_cols - len(r)) for r in all_data]

    headers = [
        "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ", "‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á", "‡πÄ‡∏ß‡∏•‡∏≤", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥",
        "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏•‡∏¥‡πà‡∏á", "‡∏Ñ‡πà‡∏≤‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏™‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö", "%‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∏‡∏ô‡πâ‡∏≥",
        "‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå", "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    ]
    if len(headers) < max_cols:
        headers += [f"‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°_{i+1}" for i in range(max_cols - len(headers))]

    file_exists = os.path.exists(CSV_OUT)
    df = pd.DataFrame(all_data, columns=headers)

    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‚Üí ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"] = df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"].apply(extract_thai)

    df.to_csv(CSV_OUT, mode="a", index=False, encoding="utf-8-sig", header=not file_exists)
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(df)} ‡πÅ‡∏ñ‡∏ß -> {CSV_OUT} (append={'yes' if file_exists else 'no'})")
    return len(df)


def main():
    all_data, t0 = scrape_waterlevel()
    n = save_csv(all_data)
    elapsed = time.time() - t0
    print(f"‚è± ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {elapsed:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ, rows={n}")


if __name__ == "__main__":
    main()
