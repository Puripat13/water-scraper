# -*- coding: utf-8 -*-
import os, time, re, sys, math
from datetime import datetime
import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

URL = "https://nationalthaiwater.onwr.go.th/waterlevel"
CSV_OUT = "waterlevel_report.csv"
SCRAPE_BUDGET_SEC = int(os.environ.get("SCRAPE_BUDGET_SEC", "600"))  # ~10 ‡∏ô‡∏≤‡∏ó‡∏µ

def make_driver():
    opt = Options()
    opt.page_load_strategy = "eager"
    opt.add_argument("--headless=new")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--window-size=1366,840")
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument("--lang=th-TH,th")
    opt.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    )
    drv = webdriver.Chrome(options=opt)
    drv.set_page_load_timeout(60)
    return drv

def find_rich_table_locator(driver):
    cands = [
        (By.CSS_SELECTOR, "div.MuiDataGrid-virtualScrollerRenderZone .MuiDataGrid-row"),
        (By.CSS_SELECTOR, "div[role='rowgroup'] [role='row']"),
        (By.CSS_SELECTOR, ".MuiTable-root tbody tr"),
        (By.CSS_SELECTOR, "table tbody tr"),
    ]
    best, best_n = None, -1
    for by, sel in cands:
        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((by, sel)))
            n = len(driver.find_elements(by, sel))
            if n > best_n:
                best, best_n = (by, sel), n
        except Exception:
            pass
    if not best:
        raise TimeoutException("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤")
    print(f"[INFO] ‡πÉ‡∏ä‡πâ locator: {best} (‡πÄ‡∏£‡∏¥‡πà‡∏° {best_n} ‡πÅ‡∏ñ‡∏ß)")
    return best

def find_paginator_root(driver):
    for sel in [".MuiTablePagination-root", "div.MuiDataGrid-footerContainer", "div[class*='MuiTablePagination']"]:
        for e in driver.find_elements(By.CSS_SELECTOR, sel):
            if e.is_displayed():
                return e
    return None

def set_rows_per_page_100(pager, driver):
    try:
        # ‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏°‡∏ô‡∏π‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤
        triggers = pager.find_elements(By.XPATH, ".//*[(@role='button' or contains(@class,'MuiSelect-select'))]")
        for t in triggers:
            try:
                driver.execute_script("arguments[0].click();", t)
                time.sleep(0.1)
                items = driver.find_elements(By.XPATH, "//li[normalize-space(.)='100']")
                if items:
                    driver.execute_script("arguments[0].click();", items[0])
                    print("[INFO] ‡∏ï‡∏±‡πâ‡∏á Rows/page = 100")
                    time.sleep(0.2)
                    return
            except Exception:
                continue
    except Exception:
        pass

def get_display_text(pager, driver):
    # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡πà‡∏ô "1‚Äì10 of 4921"
    sels = [
        ".MuiTablePagination-displayedRows",
        "*[class*='MuiTablePagination-displayedRows']",
    ]
    for sel in sels:
        es = pager.find_elements(By.CSS_SELECTOR, sel)
        for e in es:
            t = e.text.strip()
            if t:
                return t
    # ‡∏™‡∏≥‡∏£‡∏≠‡∏á: ‡∏°‡∏≠‡∏á‡∏ó‡∏±‡πà‡∏ß‡∏´‡∏ô‡πâ‡∏≤
    try:
        es = driver.find_elements(By.XPATH, "//p[contains(.,' of ')]")
        for e in es:
            t = e.text.strip()
            if t:
                return t
    except Exception:
        pass
    return ""

def parse_total_from_display(txt):
    if not txt:
        return None, None, None
    nums = [int(x) for x in re.findall(r"\d+", txt)]
    if len(nums) >= 3:
        return nums[0], nums[1], nums[2]  # start, end, total
    if len(nums) == 1:
        return None, None, nums[0]
    return None, None, None

def find_next_in_pager(pager):
    xps = [
        ".//button[@aria-label='Next page' and not(@disabled)]",
        ".//span[@title='Next Page']/button[not(@disabled)]",
        ".//button[not(@disabled) and (contains(., 'Next') or contains(., '‡∏ñ‡∏±‡∏î‡πÑ‡∏õ'))]",
    ]
    for xp in xps:
        for el in pager.find_elements(By.XPATH, xp):
            if el.is_displayed() and el.is_enabled():
                return el
    return None

def extract_cells_from_row(r):
    tds = r.find_elements(By.CSS_SELECTOR, "td")
    if not tds:
        tds = r.find_elements(By.CSS_SELECTOR, "[role='gridcell']")
    return [c.text.strip() for c in tds if c.text is not None]

def get_rows(driver, locator):
    by, sel = locator
    rows = []
    for r in driver.find_elements(by, sel):
        cols = extract_cells_from_row(r)
        if cols:
            rows.append(cols)
    return rows

def extract_thai(s):
    if pd.isna(s) or s is None:
        return ""
    m = re.search(r"[‡∏Å-‡πô].*", str(s))
    return m.group(0).strip() if m else str(s).strip()

def save_csv(rows):
    if not rows:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        return
    headers = ["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ","‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á","‡πÄ‡∏ß‡∏•‡∏≤","‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥","‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏•‡∏¥‡πà‡∏á","‡∏Ñ‡πà‡∏≤‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏™‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö","%‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∏‡∏ô‡πâ‡∏≥","‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå","‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"]
    fixed = []
    for r in rows:
        r = (list(r) + [""] * 9)[:9]
        fixed.append(r)
    df = pd.DataFrame(fixed, columns=headers)
    df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"] = df["‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ"].apply(extract_thai)
    df = df.replace({"-": "", "‚Äì": ""})
    file_exists = os.path.exists(CSV_OUT)
    df.to_csv(CSV_OUT, mode="a", index=False, encoding="utf-8-sig", header=not file_exists)
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(df)} ‡πÅ‡∏ñ‡∏ß -> {CSV_OUT}")

def scrape_all():
    d = make_driver()
    t0 = time.time()
    try:
        print(f"[INFO] ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤: {URL}")
        d.get(URL)
        WebDriverWait(d, 30).until(EC.presence_of_element_located((By.XPATH, "//*")))
        time.sleep(0.5)

        locator = find_rich_table_locator(d)
        pager = find_paginator_root(d)
        if pager:
            set_rows_per_page_100(pager, d)
            time.sleep(0.2)

        txt = get_display_text(pager, d) if pager else ""
        s, e, total_n = parse_total_from_display(txt)
        page_size = max(1, (e - s + 1) if (s and e) else 10)
        pages_plan = max(1, math.ceil(total_n / page_size)) if total_n else None
        if pages_plan:
            print(f"[INFO] paginator: '{txt}' -> total={total_n}, page_size={page_size}, pages={pages_plan}")

        all_rows = []
        curdate = datetime.today().strftime("%d/%m/%Y")
        page = 1

        while True:
            if time.time() - t0 > SCRAPE_BUDGET_SEC:
                print(f"[WARN] ‡∏´‡∏°‡∏î‡∏á‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ {SCRAPE_BUDGET_SEC}s -> ‡∏´‡∏¢‡∏∏‡∏î")
                break

            rows_elems = d.find_elements(*locator)
            first_old = rows_elems[0] if rows_elems else None

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            rows = get_rows(d, locator)
            for r in rows:
                r = (r + [""] * 9)[:9]
                r[-1] = curdate
                all_rows.append(r)

            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î IO
            if page % 20 == 0:
                print(f"[INFO] Page {page}: ‡∏™‡∏∞‡∏™‡∏° {len(all_rows)} ‡πÅ‡∏ñ‡∏ß")

            # ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            if not pager:
                print("[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö paginator -> ‡∏à‡∏ö")
                break
            if pages_plan and page >= pages_plan:
                print(f"[INFO] ‡∏Ñ‡∏£‡∏ö {page}/{pages_plan} ‡∏´‡∏ô‡πâ‡∏≤ -> ‡∏à‡∏ö")
                break

            nxt = find_next_in_pager(pager)
            if not nxt:
                print("[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏∏‡πà‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ -> ‡∏à‡∏ö")
                break

            try:
                d.execute_script("arguments[0].click();", nxt)
                # ‡∏£‡∏≠‡∏à‡∏ô‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏¥‡∏°‡πÇ‡∏î‡∏ô replace ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ loop ‡πÄ‡∏ä‡πá‡∏Ñ signature)
                if first_old:
                    WebDriverWait(d, 10).until(EC.staleness_of(first_old))
                WebDriverWait(d, 10).until(lambda x: len(get_rows(x, locator)) > 0)
                page += 1
            except Exception as e:
                print(f"[WARN] ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e} -> ‡∏à‡∏ö")
                break

        cleaned = [r for r in all_rows if re.search(r"[‡∏Å-‡πô]", r[0] if r else "")]
        return cleaned, t0
    finally:
        d.quit()

def main():
    try:
        rows, t0 = scrape_all()
        save_csv(rows)
        print(f"‚è± ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {time.time() - t0:.2f}s, ‡∏£‡∏ß‡∏° {len(rows)} ‡πÅ‡∏ñ‡∏ß")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
